apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: nccl-test-pytorch
spec:
  minAvailable: 2
  schedulerName: volcano
  queue: test-queue
  plugins:
    pytorch: [
      "--master=master",
      "--worker=worker",
      "--port=29500", 
    ]
  tasks:
    - replicas: 1
      name: master
      policies:
        - event: TaskCompleted
          action: CompleteJob
      template:
        spec: &task-spec
          containers:
            - image: nvcr.io/nvidia/pytorch:24.07-py3
              imagePullPolicy: Always
              name: nccl-test-pytorch
              command: 
               - /bin/bash
               - -c
               - |
                wget https://raw.githubusercontent.com/stas00/ml-engineering/refs/heads/master/network/benchmarks/all_reduce_bench.py
                torchrun \
                  --nnodes=2 \
                  --nproc_per_node=8 \
                  --node_rank=${RANK} \
                  --rdzv_backend=c10d \
                  --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
                  --role `hostname -s`: \
                  --tee 3 \
                  all_reduce_bench.py
              securityContext:
                privileged: true
              volumeMounts:
                - mountPath: /dev/shm
                  name: dshm
                - mountPath: /dev/infiniband
                  name: ib
          restartPolicy: Never
          volumes:
            - name: dshm
              emptyDir:
                medium: Memory
            - name: ib
              hostPath:
                path: /dev/infiniband
    - replicas: 1
      name: worker
      template:
        spec: *task-spec
