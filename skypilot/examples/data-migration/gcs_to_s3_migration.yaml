resources:
  cloud: nebius
  region: eu-west1 # https://docs.nebius.com/overview/regions
  cpus: 16 # https://docs.nebius.com/compute/virtual-machines/types#cpu-configurations

num_nodes: 2 # amount of nodes to parallel download
workdir: .

envs:
  # =================================================
  # Required variables - must be set before launching
  # =================================================
  SOURCE_BUCKET: gs://<cgs-src-bucket>
  SOURCE_BUCKET_ENDPOINT_URL: https://storage.googleapis.com
  DST_BUCKET: s3://<nebius-dst-bucket>
  DST_BUCKET_REGION: <nebius-region>
  DST_BUCKET_ENDPOINT_URL: https://storage.${DST_BUCKET_REGION}.nebius.cloud
  DST_BUCKET_ACCESS_KEY_ID: <nebius-s3-access-key-id>
  DST_BUCKET_ACCESS_KEY: <nebius-s3-access-key>
  TEMP_S3_PREFIX: gcs_to_s3_migration
  SERVICE_ACCOUNT_JSON_PATH: ./gcs-service-account.json
  # =================================================
setup: |
  echo 'Installing required tools...'

  # Install latest rclone
  curl https://rclone.org/install.sh | sudo bash

  # Install other utilities
  sudo apt-get update -qq > /dev/null 2>&1
  sudo apt-get install -y jq bc -qq > /dev/null 2>&1
  
  # Create rclone config directory
  mkdir -p ~/.config/rclone

  # Copy service account JSON file to rclone config directory
  echo "Copying service account JSON file from ${SERVICE_ACCOUNT_JSON_PATH} to ~/.config/rclone/service-account.json"
  cp ${SERVICE_ACCOUNT_JSON_PATH} ~/.config/rclone/service-account.json
  
  # Verify the file was copied
  if [ ! -f ~/.config/rclone/service-account.json ]; then
    echo "ERROR: Service account JSON file was not copied successfully"
    exit 1
  fi
  
  echo "Service account JSON file copied successfully. File size: $(wc -c < ~/.config/rclone/service-account.json) bytes"

  # Generate rclone config file
  echo "[gcs]" > ~/.config/rclone/rclone.conf
  echo "type = google cloud storage" >> ~/.config/rclone/rclone.conf
  echo "service_account_file = ~/.config/rclone/service-account.json" >> ~/.config/rclone/rclone.conf
  # s3 part
  echo "[s3]" >> ~/.config/rclone/rclone.conf
  echo "type = s3" >> ~/.config/rclone/rclone.conf
  echo "provider = AWS" >> ~/.config/rclone/rclone.conf
  echo "env_auth = false" >> ~/.config/rclone/rclone.conf
  echo "region = ${DST_BUCKET_REGION}" >> ~/.config/rclone/rclone.conf
  echo "no_check_bucket = true" >> ~/.config/rclone/rclone.conf
  echo "endpoint = ${DST_BUCKET_ENDPOINT_URL}" >> ~/.config/rclone/rclone.conf
  echo "acl = private" >> ~/.config/rclone/rclone.conf
  echo "bucket_acl = private" >> ~/.config/rclone/rclone.conf
  echo "access_key_id = ${DST_BUCKET_ACCESS_KEY_ID}" >> ~/.config/rclone/rclone.conf
  echo "secret_access_key = ${DST_BUCKET_ACCESS_KEY}" >> ~/.config/rclone/rclone.conf

  # Extract project number from service account JSON and add to config
  PROJECT_NUMBER=$(jq -r '.project_number' ~/.config/rclone/service-account.json 2>/dev/null)
  if [ -n "$PROJECT_NUMBER" ] && [ "$PROJECT_NUMBER" != "null" ]; then
    echo "project_number = $PROJECT_NUMBER" >> ~/.config/rclone/rclone.conf
    echo "Added project_number = $PROJECT_NUMBER to rclone config"
  else
    # If project_number is not available, try using project_id
    PROJECT_ID=$(jq -r '.project_id' ~/.config/rclone/service-account.json 2>/dev/null)
    if [ -n "$PROJECT_ID" ] && [ "$PROJECT_ID" != "null" ]; then
      echo "project_id = $PROJECT_ID" >> ~/.config/rclone/rclone.conf
      echo "Added project_id = $PROJECT_ID to rclone config (project_number not available)"
    else
      echo "WARNING: Could not extract project_number or project_id from service account JSON"
      echo "Will attempt to use service account authentication without project specification"
    fi
  fi
  
  #echo "location = ${SOURCE_BUCKET_REGION}" >> ~/.config/rclone/rclone.conf
  echo "storage_class = STANDARD" >> ~/.config/rclone/rclone.conf

  echo "Rclone configuration file created successfully"
  echo "Rclone config contents:"
  cat ~/.config/rclone/rclone.conf

run: |
  # Configuration and environment setup
  export TASK_ID=${SKYPILOT_TASK_ID}
  export NODE_RANK=${SKYPILOT_NODE_RANK}
  export NUM_NODES=${SKYPILOT_NUM_NODES}
  export SOURCE_BUCKET=${SOURCE_BUCKET}
  export SOURCE_BUCKET_ENDPOINT_URL=${SOURCE_BUCKET_ENDPOINT_URL}
  export SERVICE_ACCOUNT_JSON_PATH=${SERVICE_ACCOUNT_JSON_PATH}
  export SRC_DIR=${SRC_DIR}
  export DST_BUCKET=${DST_BUCKET}
  export DST_BUCKET_REGION=${DST_BUCKET_REGION}
  export DST_BUCKET_ENDPOINT_URL=${DST_BUCKET_ENDPOINT_URL}
  export DST_BUCKET_ACCESS_KEY_ID=${DST_BUCKET_ACCESS_KEY_ID}
  export DST_BUCKET_ACCESS_KEY=${DST_BUCKET_ACCESS_KEY}

  export TEMP_DIR="/tmp/gcs_to_s3_migration"
  sudo mkdir -p $TEMP_DIR
  sudo chmod 777 $TEMP_DIR

  # Verify directory creation and accessibility
  if [ ! -d "$TEMP_DIR" ]; then
    echo "Error: Failed to create temporary directory $TEMP_DIR"
    exit 1
  fi

  if [ ! -w "$TEMP_DIR" ]; then
    echo "Error: No write permission for temporary directory $TEMP_DIR"
    exit 1
  fi

  echo "Temporary directory $TEMP_DIR is available and writable"

  # Clean up any old files from previous runs
  echo "Cleaning up any old files from previous runs in $TEMP_DIR..."
  sudo rm -rf $TEMP_DIR/*
  echo "TEMP_DIR cleaned successfully"

  echo "Starting GCS to S3 migration task on node ${NODE_RANK}..."
  
  # Record start time
  START_TIME=$(date +%s)

  echo "Starting GCS migration task: ${TASK_ID}"
  echo "Running on node ${NODE_RANK} of ${NUM_NODES} nodes"
  echo "Migration started at $(date)"
  echo "Purging temporary directory ${DST_BUCKET}/${TEMP_S3_PREFIX}"
  rclone purge ${DST_BUCKET}/${TEMP_S3_PREFIX}

  # Head node lists all objects and distributes work
  if [ "${NODE_RANK}" = "0" ]; then
    echo "Head node: Listing all objects in source bucket"
    
    # Extract bucket name from SOURCE_BUCKET (remove gs:// prefix)
    BUCKET_NAME=$(echo $SOURCE_BUCKET | sed 's|gs://||')
    
    # List all objects
    rclone lsf gcs:$BUCKET_NAME --files-only --format sp --recursive > $TEMP_DIR/all_objects.txt
    
    # Count total objects and calculate estimated size
    TOTAL_OBJECTS=$(wc -l < $TEMP_DIR/all_objects.txt)
    TOTAL_SIZE_BYTES=$(awk '{sum += $1} END {print sum}' $TEMP_DIR/all_objects.txt)
    TOTAL_SIZE_GB=$(echo "scale=2; $TOTAL_SIZE_BYTES / 1024 / 1024 / 1024" | bc)
    
    echo "Found $TOTAL_OBJECTS objects with total size of approximately $TOTAL_SIZE_GB GB"
    
    # Distribute objects evenly among nodes
    echo "Distributing objects among $NUM_NODES nodes"
    
    # Split files by node using modulo on line number
    for i in $(seq 0 $((NUM_NODES-1))); do
      awk -v node=$i -v nodes=$NUM_NODES 'NR % nodes == node' $TEMP_DIR/all_objects.txt > $TEMP_DIR/node_${i}_objects.txt
      
      # Count files and total size for this node
      NODE_OBJECTS=$(wc -l < $TEMP_DIR/node_${i}_objects.txt)
      NODE_SIZE_BYTES=$(awk '{sum += $1} END {print sum}' $TEMP_DIR/node_${i}_objects.txt)
      NODE_SIZE_GB=$(echo "scale=2; $NODE_SIZE_BYTES / 1024 / 1024 / 1024" | bc)
      
      echo "Node $i assigned $NODE_OBJECTS objects with total size of approximately $NODE_SIZE_GB GB"
      
      # Verify that the object distribution file exists for this node
      if [ ! -f "$TEMP_DIR/node_${NODE_RANK}_objects.txt" ]; then
        echo "❌ ERROR: Object distribution file $TEMP_DIR/node_${NODE_RANK}_objects.txt not found!"
        echo "Available files in $TEMP_DIR:"
        ls -la $TEMP_DIR/
        exit 1
      fi
  
      echo "Processing object distribution file for node ${NODE_RANK}..."
      cut -d';' -f2 $TEMP_DIR/node_${i}_objects.txt > $TEMP_DIR/node_${i}_list.txt
  
      # Verify the list file was created successfully
      if [ ! -f "$TEMP_DIR/node_${i}_list.txt" ]; then
        echo "❌ ERROR: Failed to create file list $TEMP_DIR/node_${i}_list.txt"
        exit 1
      fi

      # Upload the file list to S3 for each node to access
      rclone copy $TEMP_DIR/node_${i}_list.txt ${DST_BUCKET}/${TEMP_S3_PREFIX}/
 
    done
    echo "Object distribution complete."
    
    # Signal that object distribution is complete by uploading to S3
    echo "done" > $TEMP_DIR/object_distribution_complete
    rclone copy $TEMP_DIR/object_distribution_complete ${DST_BUCKET}/${TEMP_S3_PREFIX}/
    echo "Object distribution complete signal uploaded to S3"
  fi
  
  # Wait for 5 seconds to ensure files are uploaded
  sleep 5

  # Each node processes its assigned files
  echo "Processing node ${NODE_RANK}"
  
  # Add extra delay for worker nodes to ensure setup is complete
  if [ "${NODE_RANK}" != "0" ]; then
    echo "Worker node: Waiting additional 3 seconds to ensure setup is complete..."
    sleep 15
  fi
  
  # Worker nodes wait for head node to finish creating object distribution files
  if [ "${NODE_RANK}" != "0" ]; then
    echo "Worker node: Waiting for head node to finish object distribution..."
    while ! rclone lsf ${DST_BUCKET}/${TEMP_S3_PREFIX}/object_distribution_complete > /dev/null 2>&1; do
      echo "Waiting for head node to complete object distribution..."
      sleep 2
    done
    echo "Object distribution complete, proceeding with copy..."
  fi
  
  # Debug: Check rclone configuration and service account file
  echo "=== Debug Info for Node ${NODE_RANK} ==="
  echo "Current working directory: $(pwd)"
  echo "Service account JSON path: ${SERVICE_ACCOUNT_JSON_PATH}"
  echo "Service account JSON exists: $(test -f ${SERVICE_ACCOUNT_JSON_PATH} && echo 'YES' || echo 'NO')"
  echo "Rclone config exists: $(test -f ~/.config/rclone/rclone.conf && echo 'YES' || echo 'NO')"
  echo "Service account JSON in rclone dir exists: $(test -f ~/.config/rclone/service-account.json && echo 'YES' || echo 'NO')"
  
  if [ -f ~/.config/rclone/rclone.conf ]; then
    echo "Rclone config contents:"
    cat ~/.config/rclone/rclone.conf
  fi
  
  if [ -f ~/.config/rclone/service-account.json ]; then
    echo "Service account JSON file size: $(wc -c < ~/.config/rclone/service-account.json) bytes"
    echo "Service account project_id: $(jq -r '.project_id' ~/.config/rclone/service-account.json 2>/dev/null || echo 'ERROR reading project_id')"
    echo "Service account project_number: $(jq -r '.project_number' ~/.config/rclone/service-account.json 2>/dev/null || echo 'ERROR reading project_number')"
  fi
  
  echo "=== End Debug Info ==="
  
  # Test rclone configuration before proceeding
  echo "Testing rclone configuration..."
  BUCKET_NAME=$(echo $SOURCE_BUCKET | sed 's|gs://||')
  
  # Test basic connectivity to GCS
  if rclone lsf gcs:$BUCKET_NAME --max-depth 1 > /dev/null 2>&1; then
    echo "✅ Rclone configuration test successful - can connect to GCS bucket"
  else
    echo "❌ Rclone configuration test failed - cannot connect to GCS bucket"
    echo "Attempting to list bucket with verbose output:"
    rclone lsf gcs:$BUCKET_NAME --max-depth 1 --verbose
    exit 1
  fi
  
  echo "Starting copy process for node ${NODE_RANK}..."
  # Download the file list from S3 to local temp directory
  rclone copy ${DST_BUCKET}/${TEMP_S3_PREFIX}/node_${NODE_RANK}_list.txt $TEMP_DIR/
  
  # Verify the file list was downloaded successfully
  if [ ! -f "$TEMP_DIR/node_${NODE_RANK}_list.txt" ]; then
    echo "❌ ERROR: Failed to download file list from S3 for node ${NODE_RANK}"
    exit 1
  fi
  
  echo "File list downloaded successfully. Starting copy process..."
  rclone copy --files-from $TEMP_DIR/node_${NODE_RANK}_list.txt gcs:$BUCKET_NAME $DEST_DIR \
  ${DST_BUCKET} --progress --links --disable-http2 --transfers=128
  
  # Check if rclone copy was successful
  if [ $? -eq 0 ]; then
    echo "✅ Copy process completed successfully for node ${NODE_RANK}"
  else
    echo "❌ Copy process failed for node ${NODE_RANK}"
    exit 1
  fi
  
  # Signal completion by uploading to S3
  echo "done" > $TEMP_DIR/node_${NODE_RANK}_done
  rclone copy $TEMP_DIR/node_${NODE_RANK}_done ${DST_BUCKET}/${TEMP_S3_PREFIX}/
  echo "Node ${NODE_RANK} completion signal uploaded to S3"

  # Head node verifies all nodes have completed
  if [ "${NODE_RANK}" = "0" ]; then
    echo "Head node: Verifying own copy process completed successfully..."
    
    # Verify head node's own completion file exists on S3
    if ! rclone lsf ${DST_BUCKET}/${TEMP_S3_PREFIX}/node_0_done > /dev/null 2>&1; then
      echo "❌ ERROR: Head node completion file not found on S3! Copy process may have failed."
      exit 1
    fi
    echo "✅ Head node copy process verified as complete"
    
    echo "Head node: Waiting for all worker nodes to complete"
    
    # Wait for all workers to complete
    for worker in $(seq 1 $((NUM_NODES-1))); do
      echo "Waiting for worker $worker to complete..."
      while ! rclone lsf ${DST_BUCKET}/${TEMP_S3_PREFIX}/node_${worker}_done > /dev/null 2>&1; do
        echo "Waiting for worker $worker completion signal on S3..."
        sleep 2
      done
      echo "Worker $worker has completed"
    done
    
    echo "All workers have completed. Verifying object counts..."
    
    # Count objects in source and target buckets
    # List source bucket files
    rclone lsf gcs:$BUCKET_NAME --recursive --files-only > "$TEMP_DIR/final_source_files.txt"
    rclone lsf ${DST_BUCKET} --recursive --files-only > "$TEMP_DIR/final_target_objects.txt"
    
    # Filter out directory entries and temp files
    TEMP_DIR_NAME=$(basename "$TEMP_DIR")
    grep -v "^${TEMP_DIR_NAME}/" "$TEMP_DIR/final_target_objects.txt" > "$TEMP_DIR/final_target_files.txt"
    #grep -v "^$TEMP_DIR/" $TEMP_DIR/final_target_objects.txt > $TEMP_DIR/final_target_files.txt
    
    # Extract just the object keys for comparison
    cat $TEMP_DIR/final_source_files.txt | awk '{print $NF}' | sort > $TEMP_DIR/source_keys.txt
    cat $TEMP_DIR/final_target_files.txt | awk '{print $NF}' | sort > $TEMP_DIR/target_keys.txt
    
    # Compare counts
    SOURCE_COUNT=$(wc -l < $TEMP_DIR/final_source_files.txt)
    TARGET_COUNT=$(wc -l < $TEMP_DIR/final_target_files.txt)
    
    # Calculate and print migration duration
    END_TIME=$(date +%s)
    DURATION=$((END_TIME - START_TIME))
    HOURS=$((DURATION / 3600))
    MINUTES=$(( (DURATION % 3600) / 60 ))
    SECONDS=$((DURATION % 60))
    
    echo "------------------------------------------------------"
    echo "📊 Migration Statistics:"
    echo "Total migration time: ${HOURS}h ${MINUTES}m ${SECONDS}s"
    echo "Source bucket: $SOURCE_COUNT objects"
    echo "Target bucket: $TARGET_COUNT objects"
    echo "------------------------------------------------------"
    
    if [ $SOURCE_COUNT -eq $TARGET_COUNT ]; then
      echo "✅ Migration completed successfully! Object counts match."
      
      # Even if counts match, check for differences in file lists
      if diff $TEMP_DIR/source_keys.txt $TEMP_DIR/target_keys.txt > $TEMP_DIR/diff_output.txt; then
        echo "✅ All objects match between source and target."
      else
        echo "⚠️ Warning: Although counts match, some objects differ between buckets."
        echo "Objects in source but not in target (first 10):"
        grep "^<" $TEMP_DIR/diff_output.txt | head -10
        echo "Objects in target but not in source (first 10):"
        grep "^>" $TEMP_DIR/diff_output.txt | head -10
      fi
    else
      echo "⚠️ Migration completed with warnings. Object counts don't match."
      echo "Source: $SOURCE_COUNT, Target: $TARGET_COUNT"
      
      # Show detailed differences
      echo "Analyzing differences between source and target buckets..."
      
      # Find files in source but not in target
      comm -23 $TEMP_DIR/source_keys.txt $TEMP_DIR/target_keys.txt > $TEMP_DIR/missing_in_target.txt
      MISSING_TARGET=$(wc -l < $TEMP_DIR/missing_in_target.txt)

      echo "$MISSING_TARGET files exist in source but not in target"
      
      if [ $MISSING_TARGET -gt 0 ]; then
        echo "Sample of missing files in target (first 10):"
        head -10 $TEMP_DIR/missing_in_target.txt
      fi
      
    fi
    
    # Clean up temporary dir
    #rm -rf $TEMP_DIR
      
  fi
  # end
